# 分布式学习
<!-- TOC -->

- [分布式学习](#分布式学习)
  - [一、选举 - Leader](#一选举---leader)
  - [二、复制 - Replicate](#二复制---replicate)
  - [三、安全 - Safety](#三安全---safety)
  - [四、集群成员变化](#四集群成员变化)
  - [五、Log压缩](#五log压缩)
  - [五、Client请求](#五client请求)
  - [学习资料](#学习资料)

<!-- /TOC -->
Raft主要关注的三个方面，
* Leader选举
* log复制
* 安全问题
解决好上面三个问题，一个基本的分布式系统就可以做好了。
## 一、选举 - Leader
其实在阅读[Raft网站](https://raft.github.io/)的时候有一些疑问，这里主要从选举、复制（目前只是知道这两个方面）。

首先列举选举相关问题：
* 选举一问：因为Raft集群服务器存在三种角色，Follower、Leader、Candidate。Follower和Leader是选举之后的状态，那么当一个Leader宕机之后。各个Server的角色是什么样的呢？比方说之前的Follower角色，现在会变成什么样的角色呢？
  
  选举一答：在Raft论文中，一个Server在一个Cluster中的最开始的角色时Follower角色，我们假定在一个选举完成的状态下，因为Leader会以一个固定的频率给集群中的Follower发送心跳检测，如果Follower在其设定的timeout超时时间之前还没有收到Leader的心跳检测，此时Follower就会进入Candidate角色；Candidate角色开始向集群中其他的Server发送选举请求（这里的其他Server是包括Leader角色的Server），如果在一个选举周期内，没有投出一个有效的Leader（这里一个周期内，如何算是投出一个有效的Leader，我们后面做出说明），此时Candidate角色的Server的角色保持不变，进入下一轮投票；如果Candidate角色的Server在投票期间获得了Cluster中大部分的Server的投票，那么Candidate角色就会转变为Leader角色；Follower角色又是如何发生变化的呢？Follower角色在收到一个比自己term更大的请求的时候，就会自然的从Leader角色转变为Follower角色。当然，上述的状态变化没有涉及到更多的细节，以及还存在一些疑问，我们把这些在学习过程中遇到的一些疑问，我们列举在下方，并成为一个问题继续探索。

* 选举二问：在上面我们提到一个选举周期，在一个选举周期呢？
  
  选举二答：Raft在论文中提到吧time分割成不同的term，每个term的时长是不一样的，在每一个term开始时都需要进行选举，选举完成之后开始响应Client的请求。如果自一个term中没有选举出合适的Leader（一个集群中超过一半的Server都投这个Server作为Leader），那么此时term就会结束，同时集群中的每一个Server的term在前一个term基础上自增。

  这里也提到，集群中的Server在通信时会传递自己的term，如果一个Server发现自己的term比其他Server的term要小，那么这个Server就需要升级自己的term。如果一个Server收到请求的term，这里在文中被称为stale term（过期term）， Server会拒绝请求。

  虽然我觉得这种方式不大靠谱，因为每个term都有选举，选举完成之后才能服务Client的请求，这个在现实的互联网世界，基本上时不可能的。那么实际的数据又是怎样的呢？，可以看一下 ETCD、Redis的设计方式。

* 选举三问：如果在一个term中，一个 Follower未收到一个Leader的心跳检测，那么之前为Follower角色的Server就会转变为Candidate角色。此时Server只是一个角色的转换吗？会开启一个新的term吗，因为Candidate此时会向集群中其他的Server发出选举请求。在文中提到一句一个Candidate或者Leader如果发现自己的term已经过时，此时Candidate、Leader就需要转变成Follower角色？
  
  选举三答：一个如果在一个周期内（election timeout）没有收到Leader的心跳检测，此时Server就会从Follower角色转变为Candidate角色，同时升级自己的term。

  成为Candidate角色之后，该Server会给自己投一票，同时会向Cluster中其他的Server发送投票，此时应该也会带上自己的term。在选举二答中提到term的同步时通过服务器之间的通信会带上自己的term，同时其他Server在发现自己的term小于发起请求的Server的term就会升级自己的term，同时一个Leader或者一个Candidate发现自己的term小于其他Server的term时会从之前的角色转换成Follower角色。因此Leader在这个时候就会转换成Follower，转换之后的Follower就会响应Candidate的请求。


* 选举四问：因为是一个分布式系统，没有一个集中的决策机制，正是通过这种机制，那么一个Follower又是如何决定要给一个Candidate投赞成票还是投反对票。
  
  选举四答：如果一个Candidate能够在集群中的同一个term的选举中获得大部分的Server的投票，那么这个Candidate就能够称为一个Leader。这里Follower如何给一个Candidate投票，可以在阅读了后main的内容之后再来回答这个问题。读了3.6Safety这一章节之后可以来回答了，比方说：

* 选举二问：当选举中是否会出现多个Leader（即分布式中著名的拜占庭将军问题），同时多个Leader是否最终能够由一个Leader来统领。Raft算法是如何解决词问题的。

* 选举三问：当一个Candidate成为了Leader之后，那么其他的Candidate是如何确认了自己的Follower身份的。
  
* 选举X问：我们提到一台Server在其 timeout时间内没有收到Leader发送过来的心跳检测就会自动转换成为一个Candidate角色，那么这个Server的timeout是如何合理决定的呢？

* 选举四问：选举中Candidate发出的请求和选举完成之后发出的心跳保活请求是否是同一个类型，那么Candidate和Follower角色的Server是如何做出区分的，同时又是如何回应的。

* 选举五问：选举中Candidate发出的请求和选举完成之后发出的心跳保活请求是否是同一个类型，那么Candidate和Follower角色的Server是如何做出区分的，同时又是如何回应的。

  选举五答：在Raft的原文中提到了两种Rpc，一种是RequestVote主要是在当一个集群中的Follower无法感知到Leader时，此时Follower的角色就会转换成Candidate，此时就会使用RequestVote类型的Rpc与集群中其他Server进行通信。这个通信的目的是让其他 Follower知道自己已经是Candidate，投自己一票。还有一种是Entries Rpc，这种Rpc请求的方式就是Server将自己的log同步给Follower。

* 选举问题六：那么这个时候衍生出一个问题，当一个集群正常运行时，集群中的Follower是如何知道其他 Follower的地址的。
  
  选举六答：我想应该是当一个Candidate在成为Follower的同时，应该会发送一个Rpc请求，将集群中存活的所有Server的IP和Host同步给集群中所有的Follower。那么这中Rpc使用的是RequestVote Rpc还是使用Entries Rpc呢？如果是让我来设计，我应该如何设计呢？我想我会创建一个新的Rpc来解决这个问题。

* 选举问题七：加入一个集群中有5台Server，如果其中的Leader已经挂掉了，同时另一台 Follower也挂掉了，那么此时进入选举环节，其中剩余的每一台Server都会从Follower转换成Candidate角色。此时每一个Candidate都会给集群中的其他Server发送Vote Rpc（因为Candidate只知道Leader宕机了，并不知道另一台Server也宕机了，因此会发送出4个请求），此时Candidate首先会投自己一票，然后剩余的两票来自其他的Server，那么这个时候这个Candidate只得到的三票。这个时候这个Candidate还是可以获胜的，因为可以理解为5票中获取了3票，依旧是获取了集群中大部分集群的投票，此时Candidate会成为Leader。假设 Leader宕机的同时，另外还有两台Server也宕机了，此时两个Candidate会出现的情况是各自获得一票，或者是获得两票，那么此时Candidate会成为Leader吗？如果能够成为Leader是不是就默认集群中所有的Server都能感知到其他Server已经宕机的了，如果不是这种方式，那么又是如何解决的。
  
  这里从[Raft](https://raft.github.io/)官网可以看到这个如果集群中有五台Server，那么一旦有其中的三台Server宕机了，那整个集群就会陷入到无尽的选举过程，因为每一个Candidate都无法获得整个集群中半数以上（≥3）的投票，因此剩下的Server都会成为Candidate，成为Candidate的Server会不断的升级自己的 term。

* 选举问题八：如何防止一个Follower在选举期间只给其中一个Candidate投票，而不是给多个Candidate投票。

## 二、复制 - Replicate


* 复制一问：当一个集群在一个term开始阶段经历了选举完成，并选举出了一个Leader，那么此时Leader是如何接受Client的请求的，同时又是如何保证Client的请求在Leader和Follower机器上保证一致性。
  
  复制一答：Raft算法中，每台机器中包含一个 log文件，另外一个是state machine。log文件主要用于存储client的请求（这里暂且这样表述，因为在后面的集群中的配置也是存在log文件中），state machine主要用于执行log文件的命令。其中Leader在收到Client的请求时，首先是将用户请求中的Command写入到log文件中，Leader在写自己log文件的同时，也会通过AppendEntries RPC请求将Command复制给集群中的Follower，这里文中只是提到了将Follower将Command写入log文件之后就返回响应给Leader，并没有提到Follower在什么时候执行log文件（还是说这里follower并不执行log中的command，只有Leader会执行 log中的command）。Leader在收到Follower的响应之后，就开始让state machine执行log文件中的command，并将执行到的结果返回给Client。

  
* 复制二问：集群中只有一个Leader，有多个Follower，那么多个Follower是全部都写入log之后Leader才执行command还是只要集群中的半数以上的Follower已经将command写入log就可以执行Command然后返回结果。
  
  复制二答：Raft Thesis中提到log中的一个entry被state machine执行后称之为commit（提交）。 log中每一个entry包含的信息有command和term。Leader只要集群中大多数的Follower已经复制了entry之后，就会提交这个entry（也就是将entry交给 state machine执行）。

* 复制三问：响应一个Client的请求分为三个步骤，Leader首先写自己的log，然后Leader将entry同步给Follower，Follower将entry写入到自己的 log；第三步Leader将entry提交到state machine执行，第四步Follower了解到Leader已经提交了entry，那么此时Follower也会将这个entry在自己的state machine中执行。
  
  这里每一步都有可能出错，如果第一步Leader在写自己的log成功之后，此时Leader宕机；在第二步一部分Follower将entry写入到log之后，此时Leader宕机；在第三步，大部分Follower将entry写入到log之后，此时Leader使用state machine执行command之后宕机；第四步，一部分机器在提交entry的时候成功，一部分机器执行entry失败，在后续的选举或者是同步过程中，应该如何处理这部分数据。这里也可以看到在分布式系统中，一个步骤被拆的越详细，出问题的概率也会越大。
  
  针对这几个问题，会涉及到后续Leader选举时应该选谁当Leader问题（因为一部分 Follower已经将entry写入log当中），选举完成之后Leader与Follower之间的 log同步问题（因为一部分Follower写人了这个entry，而另外一些Follower并没有写入）；第三个问题就是Client请求的幂等性问题（如果Leader在提交之后宕机，那么下一次client在请求的时候就会很迷惑，比方说现在客户看到了x = 1,在刚才宕机的时候要求执行x++，此时Leader提交了但是Leader宕机了，那么客户看到的就是）。

* 复制四问：当一个集群中有半数的机器已经复制了Leader发送的entry，此时Leader就会将Entry提交给State Machine执行，并将执行后的结果返回给Client，这个过程称为Committed。当Follower知道Leader已经将entry提交之后，就会将entry提交给


## 三、安全 - Safety


## 四、集群成员变化
## 五、Log压缩
完成Raft第五章的学习并把第四章类容回顾一下

* 问题一：什么是Log compaction？

  * 回顾一下今天的主题，为什么要做Log Compaction，是因为当Leader接受到更多了Request之后，每一台Server要存储的Log会变得越来越多，如果是存储在内存当中，随着Client请求越来越多，需要存储的entry会变多，内存有限。一旦entry被提交之后，其实entry就已经不再被需要需要将一部分历史数据通过Compaction存储到disk。
  * 什么是 Log Compaction呢？简而言之，就是Raft中的一些entry已经过时了，需要丢弃掉，只需要保存系统的最终状态就可以。
  * 什么是snapshotting？将Log中一段时间的entry通过序列化的方式写入到 disk后，将中间状态丢弃，然后只存储本次压缩后的系统的状态以及log的位置、集群最新的配置信息。其中state machine负责将Raft中的log从内存中序列化后写入到disk当中。同时Raft需要保存本次写入到disk的最后一个entry的index、term、系统最终状态、集群成员配置等信息。
  
  * 我们详细讲一下 Log compaction，我们假设log的entry为(index = 1, term = 1, x = 1),（index = 2, term = 1, x = 2），(index = 3, term = 2,x = 3)，(index = 4, term = 3, x = 5)，(index = 5, term = 4, x = 7)，如果Raft决定在index = 4时执行log compaction，此时Raft就会将log复制到state machine当中，然后state machine负责将index = 1位置至index = 4位置的数据序列化后写入到disk，然后Raft存储一个数据，这个数据就是index = 4这个位置时的系统最终态（我们这里只列举了index、term、x，这里的变量只有x，其实在真实的分布式系统中会有很多，我们这里也忽略了Cluster Member Info）。然后Raft就会将index = 4 这个位置和之前的数据全部删除，log中剩下的就只有(index = 5, term = 4, x = 7)。下一次要执行log compaction的时候，只需要从这个位置开始就可以的(其实这里也有点想不明白，如果采用array的话，那么就算前面的数据已经被删除了，那么删除了的entry的数组空间能够被释放吗，这里还是存有疑问的，如果只是单纯的删除了元素，并没有释放空间，那么这个操作的意义是啥呢？这里在具体实现中希望能够找到一个解答)。
  
  * 这里具体的流程和细节可以参考Raft thesis论文中关于Log Compaction的细节，我们这里主要关注序列化的几个问题，一个是Log Compaction一方面需要占据State Machine的执行，那么如何保证在能够完成client请求的情况下，同时能够完成log compaction。第二个问题就是Raft应该在什么时候开始执行Log compaction？第三个问题是序列化的数据如何写入到Disk中，因为后续存在slow follower（集群中复制比较慢的Server）和new Server（新加入到集群中的Server），这些Server在复制Leader的log时，Leader可能已经把这些Server要复制的index位置的entry已经从log中删除了，那么如何从之前log compaction操作写入到disk的snapshot（就是序列化好的一段时间的entry集合）恢复出来，并将这些entry能够同步给slow Server和new Server。
  * 这里还有一点值得提出的就是，想比较log replication是从Leader同步entry到Follower，log compaction操作是每个Follower自己决定的，只有一个原则，机器能够在宕机重启后恢复出宕机之前已经复制好的entry和state就可以。

* 问题二：对于Leader中已经删除的log，那么针对 slow Server和new Server应该通过同步呢？
  * 在前面关于Cluster MemberShip Change的时候，Leader一方面需要将集群信息写入log然后通过AppendEntry RPC同步给集群中其他Server（Follower）。同时新加入的Server如果想要加入Cluster，那么需要确保这个Server能够足够快的复制entry。现在出现log compaction，Leader只保留最新的system state（系统各个变量的最终值），此时如果new Server想要从Leader中复制这个index之前的entry，那么只能从disk中找到包含这个index的snapshot，然后将反序列化后对应index位置的entry同步给new Server和slow Server。

* 问题三：如果Leader已经删除了自己的log，同时将历史序列的entry已经序列化写入到disk了，此时如果机器宕机了，服务重启之后应该如何恢复之前的状态呢？比方说log信息如何恢复？
  
  * 这里可以将log分为两种情况，如果一个 log的数据已经全部写入到disk了，那么是否需要加载所有的 snapshot之后才能复现宕机之前的state呢？我想是不用的，因为如何这么做的话，这个系统设计就太傻了。
  * 如果宕机的时候，log有一部分在h还没有删除，一部分已经写入到disk了，那么这一部分就需要怎么操作呢，因为log中已经存在的数据能够保证是服务器当前的状体。


* 问题三：什么时候将log中的entry序列化并将序列化的数据写入到disk当中？
  
* 问题四：应该以何种数据结构存储序列化的数据并且能够实现快速写入和读出数据？
  
  * state machine将log存储在内存中，将log在某一个时间点之前的entry全部写入到stable storage上，将已经写入的entry从现在log上删除。
  
  * 有的state machine是将状态存储在disk上，一旦这个raft的log已经写到磁盘上，那么这个log就可以被删除。
  

  * Raft负责将某一个时间节点的log的prefix（之前的所有entry）转移到 state machine（这里的转移可以理解为复制）。state machine负责将这些entry写入到disk（其实是需要将这些entry完成序列化，然后写入disk），有了这个操作，一旦机器宕机了，也可以从disk中读取当前的系统的state，从而恢复机器的状态。Raft在放弃之前的entry之前，需要将之前的entry进行一个压缩，比方说中间的entry有x=3,但是在log Compaction节点时刻最后一个entry的状态是x = 5, 此时log就会将存储x = 5，之前的状态全都丢弃。我很好奇，如果Raft保存这个信息，那么Raft保存在哪里呢，是不是说这个其实就是一个Flag，告诉下一次执行log compaction的时候需要从何处开始。我之前以为的log compaction是将最后的状态写入的磁盘，其实这里是有一个误区的，写入到磁盘的是log entry未被压缩的信息，Raft是在entry写入磁盘后，将某一个节点的数据全部清除，然后保存最后状态便于下一次执行log compaction能够找到位置。
  * 之前在提到Cluster Configuration Change的时候会把 Cluster Information存储在log上面，Log Compaction之后Raft也会保留最新的Cluster Membership Information；
  
  * Raft一旦log compact，就会将之前的entry丢弃，state machine就需要承担两个事情，一个是服务重启时，state machine需要从磁盘中加载之前log compaction 操作执行的时候存储的system state信息恢复log。
  * 一个集群中可能会出现slow follower，这些follower复制entry的速度比较慢，如果Leader已经执行了log compaction，那么这些较慢的follower在执行log replication的时候会出现这个entry在Leader侧已经丢弃（因为Leader已经执行了log compaction）。那么state machine此时就需要将存储在disk的state重新恢复出来，然后让slow follower能够完成 log replication（其实这个slow follower大概率是新加入集群的Server）
  * 其实一个log compaction，将数据存储到disk，也是需要考虑到存储数据的数据结构。如果一个slow follower需要执行一次同步但是Leader的log已经删除，那么state machine就需要快速的从已经存储在disk找到对应的信息。所以这个数据结构要有很好的查找和写入的能力，而不能只是单纯的将其写入，而查找却耗费大量的时间。
  * 一旦state machine完成了写入一个snapshot完成，那么log就可以丢弃，Raft就需要存储snapshot时刻的index和term，以及这个位置是每个变量的最新的状态，以及最新的集群配置信息。
  * snapshoting这里有个问题就是一方面需要解决的问题就是应答client和执行序列化和写入disk需要并行执行。而且在执行序列化的时候有可能将内存全部用完的情况。
  * 服务器什么时候执行snapshotting呢？一旦当前的log的大小已经超出了前一个snapshot文件一定的量级之后就需要执行快照存储了。
## 五、Client请求
论文作者把用户请求集群的细节放到了论文的最后部分解答，因为这个不是这个分布式系统最需要关注的问题。Client请求可以简单的理解为两台机器之间的通信，那么只需要知道两台机器的IP和host，那么两台Server就可以建立连接。那么为什么这样一个简单的事物化为什么会单独拿出一章来讲这个看似简单的问题呢？一开始以为这一章只是一个简单的问题，其实这一章和我们工作中接触到的Mysql、Redis等分布式数据存储都是息息相关的。如果能够很好的回答这些问题，其实对于分布式系统大多数问题都能够得到解答。

* 如果一个集群是不变的，同时Leader的信息不发生变化的情况下，那么这个问题就简化了，因为只要Client和Server之间建立连接就可以通信了。现实情况是集群的Leader会发生变化，因此Client需要保证自己能够和Cluster进行通讯。
  
* 一个集群中Cluster 成员会发生变化，因此我们实际上在连接的时候从来使用的不是ip:host方式进行连接的，而是使用域名的方式进行连接。那么业务中使用域名方式又是如何解决集群中Leader信息发生变化的的？这个问题其实也要好好的看一下。

* client与Cluster集群之间建立连接后，需要解决的另外一个问题如何保证Client的一个请求只会被commit一次，而不会被执行两次，执行两次是什么情况呢？如果Client第一次和Leader进行交互，如果此时Leader已经让集群中大多数Server复制了entry（针对这次请求的entry），同时Leader也将entry提交给了state machine，但是在state machine执行完成之后Leader宕机，此时无法回应用户的请求。当集群下一次选出一个新的Leader，Client与new Leader重新建立连接，此时Client再一次请求执行之前的操作， new Leader会把之前已经执行过的command重复执行一次，相当于client只想针对变量x = 1自增1，但是因为服务器的崩溃导致服务器执行了两次，此时x = 3了。Raft还需要解决写一致性。
* 如果一个分布式系统存在写一致性，那么也会存在读一致性，这个问题应该如何解决呢？


## 学习资料
[How I am learning distributed systems](https://medium.com/@polyglot_factotum/how-i-am-learning-distributed-systems-7eb69b4b51bd)