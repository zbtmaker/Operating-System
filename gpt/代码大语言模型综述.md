# 代码大语言模型综述<!-- TOC -->

- [代码大语言模型综述](#代码大语言模型综述)
  - [Abstract](#abstract)
  - [Introduction](#introduction)
  - [背景](#背景)
    - [Casual Language Modeling](#casual-language-modeling)
    - [Masked Language Modeling](#masked-language-modeling)
    - [Denoising Objectives](#denoising-objectives)
    - [Auxiliary Objectives](#auxiliary-objectives)
    - [Implementation Design](#implementation-design)
  - [Evaluation of Language Models for Code](#evaluation-of-language-models-for-code)
    - [Downstream Tasks of Code Pressing](#downstream-tasks-of-code-pressing)
      - [Text-to-Code](#text-to-code)
      - [Code-to-Code](#code-to-code)
      - [Code-to-Code](#code-to-code-1)
      - [Code-to-Patterns](#code-to-patterns)
      - [Text-to-Text](#text-to-text)
      - [NLP Point-of-View](#nlp-point-of-view)
    - [Evaluation Metrics](#evaluation-metrics)
    - [Program Synthesis](#program-synthesis)
    - [Repository-Level Evaluation](#repository-level-evaluation)
  - [General Language Models for Code](#general-language-models-for-code)
    - [Off-Shelf Lanaguage Models](#off-shelf-lanaguage-models)
    - [Language Models with Additional Pretraining on Code](#language-models-with-additional-pretraining-on-code)
  - [Specialized Language Models for Code](#specialized-language-models-for-code)
    - [Training Dataset for Code](#training-dataset-for-code)
    - [Encoders](#encoders)
    - [Encoders-Decoders](#encoders-decoders)
    - [Decoders](#decoders)
    - [UniLMs](#unilms)
    - [Diffusion Model](#diffusion-model)
    - [Instruction Finetuning and Reinforcement Learning for Code](#instruction-finetuning-and-reinforcement-learning-for-code)
  - [Code Features for Language Models](#code-features-for-language-models)
    - [Abstract Syntax Tree and Intermediate Representation](#abstract-syntax-tree-and-intermediate-representation)
    - [Control Flow and Data Flow](#control-flow-and-data-flow)
    - [Type](#type)
  - [7 LLMs in Software Developement](#7-llms-in-software-developement)
    - [7.1 LLMs Extended with Coding Tools](#71-llms-extended-with-coding-tools)
    - [7.2 LLMs Integerated into Software Development](#72-llms-integerated-into-software-development)
  - [8 Conclusion and Challenges](#8-conclusion-and-challenges)

<!-- /TOC -->
## Abstract
在这项工作中，我们系统地回顾了语言模型在代码处理方面的最新进展，涵盖了50多个模型、30多个评估任务和500项相关工作。我们将代码处理模型分解为以GPT系列为代表的通用语言模型和专门针对代码进行预训练的专用模型，通常具有定制的目标。我们讨论了这些模型之间的关系和差异，并强调了代码建模从统计模型和RNN到预训练的Transformers和LLM的历史转变，这与NLP所采用的过程完全相同。我们还讨论了AST、CFG和单元测试等特定功能，以及它们在训练代码语言模型中的应用，并确定了该领域的关键挑战和潜在的未来方向。我们在github存储库上保持调查的公开性和最新性athttps://github.com/codefuse-ai/Awesome-Code-LLM.

## Introduction
近年来，随着BERT（Devlinet et al.，2019）和GPT（Radford et al.，2018）等预训练变换器（Vaswani et al.，2017）的出现，语言建模取得了显著进展。随着大型语言模型（LLM）扩展到数十亿个参数，并开始显示通用人工智能的早期迹象（Brown et al.，2020；Chowdhery et al.，2022；OpenAI，2023），它们的应用也超越了文本编程。在Codex（Chen et al.，2021）的引领下，LLM在代码处理方面取得了令人印象深刻的成果，产生了GitHub Copilot1等商业产品和StarCoder（Li et al.，2023）和code LLaMA（Rozière et al.，2022）等开源数十亿码模型。

然而，预训练变压器在代码处理中的应用可以追溯到1https://github.com/features/copilotdates在解码器之前，仅自回归模型占主导地位（Feng et al.，2020；刘等人，2020），该领域尚未进行全面综述。为了弥合自然语言处理（NLP）社区和软件工程（SE）社区之间在语言模型应用主题上的差距，我们对本工作中的代码语言模型进行了全景式的调查，涵盖了50多个模型、30多个下游任务和500个相关工作。我们细分了不同类别的代码语言模型，从在通用语言上训练的巨大模型到专门为代码理解或生成训练的微小模型。我们强调这些模型之间的关系和差异，并强调将特定于代码的特征（如抽象语法树或数据流）集成到语言模型中，以及从NLP中提取的最新技术。

与我们的工作相关，我们知道有几项关于类似主题的调查，其中三项工作与我们一致（Hou et al.，2023；郑等人，2023年；Sheet等人，2023.）。然而，这些工作要么集中在NLP方面（Zan et al.，2023；Xu和Zhu，2022），要么集中在SE方面（Niu et al.，2022；Hou et al.，2021；Zheng et al.；2023；She et al.。2023），没有涵盖来自另一方的模式、任务和挑战。例如，Zan等人（2023）专注于文本到代码生成的LLM，而很少讨论软件工程社区中的其他评估任务。相比之下，Hou等人（2023）和She等人（2021）全面回顾了ASE和ICSE等SE场所的工作，但仅引用了ACL、EMNLP、NeurIPS和ICLR等深度学习和NLPvenues的少数工作。


这项工作的其余部分按照图1所示的轴突切开术进行组织。在§2中，我们首先提供了语言模型和Transformer模型的初步内容，然后在§3中，我们对代码的语言模型进行了文本化评估，强调了从各种代码理解任务到更实际的代码生成任务的历史转变。在§4中，我们讨论了已证明可编码性的LLM的可能性，然后在§5中，我们根据其架构审查了专门的和通常较小的模型，特别关注填充目标、教学调整、强化学习和工程改进的最新应用。然后，在第6节中，我们讨论了代码的独特特征，这些特征是自然语言不可用的，但已被用于帮助代码处理。在§7中，我们回顾了LLM和软件开发之间的最新集成，最后在§8中结束了这项工作，并强调了代码处理中的当前挑战。

## 背景
在本节中，我们简要回顾了基于Transformer的语言建模的初步内容，包括单向和双向模型的共同目标，以及NLP中一些流行的模型和设计。

### Casual Language Modeling
单向语言模型（也称为因果语言模型2）将句子的概率因子为每个标记的条件概率与链式规则的乘积。

一段输入文本x=[x1，x2，··，xn]由22组成，这些语言模型的训练目标是因果语言建模（CLM），但也被称为下一个标记预测。,令牌被建模为P（x）=nYi=1pθ（xi |x1:i−1），（1）其中x1:i–1是输入前令牌的缩写，θ是模型的参数。使用诸如GPT（Radford et al.，2018；Radford等人，2019；Brown等人，2020）和LLaMA（Touvron et al.，2023；Touvronet等人，2023）的Transformer解码器，通过将注意力掩码添加到每个Transformer块的注意力矩阵来建模（1）中的条件概率，从而确保Xican只关注先前的令牌。在训练过程中，输入中所有令牌的交叉熵损失是并行计算的，而在推理时，新令牌是自回归生成的。有关变压器架构的更多详细信息，请参阅Vaswani等人（2017）。

### Masked Language Modeling
与 causal language modeling不同，bidirectional language modeling被训练来获得更好的文本表示，而不是自回归生成文本。在普通的Transformer中，编码器部分可以为此目的处理令牌的左上下文和右上下文。BERT（De vlin等人，2019）更进一步，只训练了一个变压器编码器。将输入中的一个集合Mof个随机选择的令牌替换为一个特殊令牌[MASK]，以获得一个有噪声的输入ξx，例如[[CLS]，x1，[MASK]x3，[MASK]，x5，[EOS]]3，并且通过最大化Ym∈Mpθ（m|ξx）来训练模型以恢复原始令牌。

虽然这一目标要求模型对要重建的输入文本有深入的理解，但它的训练效率很低，因为只有一小部分标记（通常为15%）被屏蔽（并且“训练”）。为了解决这个问题，Clark等人（2020）提出了ELECTRA，它被训练来区分输入中的每个令牌是否被类似BERT的模型取代。

### Denoising Objectives
GPT型因果LM和BERT型双向LM各有优缺点。虽然GPT可以用于自回归生成，但它缺乏输入文本的双向表示，因此不适合于序列到序列（seq2seq）的生成任务，如转换和摘要。另一方面，BERT可以产生双向表示，但仅针对掩码填充而非生成进行重新训练。

vanilla Transformer编码器-解码器架构结合了GPT和BERT各自的优点。T5（Raffel et al.，2020）就是这样一个用跨度腐败预训练的模型，它可以被视为传销的一种变体。在预训练期间，输入中的文本跨度被哨兵标记替换，哨兵标记在BERT中扮演与[MASK]相同的角色。带噪声的输入首先由具有双向注意力的编码器进行处理，然后由解码器自动生成掩蔽跨度。形式上，如果对kspan的输入x中的损坏进行采样，则通过用特殊标记<extra_id_i>替换每个跨度来构建无输入的vx轴，对于i=1,2，··，k，并且通过将所有跨度连接起来来构建目标，这些跨度用相应的哨兵预处理：[<extra_id_1>，span1，···，<extra_id_k>，打屁股]。然后用标准seq2seq目标训练模型，通过最大化pθ（y|trx）=nyYi=1pθ（yi|trx，y1:i−1）。3.</extra_id_k></extra_id_1></extra_id_i>


Lester et al.(2021)证明了用这些目标预训练的模型可以适用于自动渐进语言建模，而对前缀语言建模目标进行额外的预训练，即将文本分成两部分，用编码器处理第一部分，用解码器生成第二部分。

Tay等人（2023）认为，跨度损坏也与CLM密切相关，因为可以将整个输入文本屏蔽为单个跨度，并训练编码器自回归生成。受这种关系的启发，他们提出了UL2，它是腐败率和跨度长度不同的许多跨度腐败目标的组合。将其应用于编解码器模型和纯解码器模型，发现在相同的计算芽约束下，编解码器模型表现得更好。其他研究也发现,这种编码器-解码器模型通常每种形式都比仅因果解码器模型好（Wang等人，2022；Soltan等人，2022）。

### Auxiliary Objectives
语言建模目标，如前面讨论的CLM和MLM，主要训练模型来捕获令牌级信息，并且在建模文档结构方面无效。因此，通常会添加辅助目标来帮助模型学习此类全局信息。BERT是通过下一句预测（NSP）和MLM进行预训练的，MLM被公式化为一个二元分类任务，用于预测输入中的两个片段在原始语料库中是否相邻。Lanet al.（2020）提出了一个更具挑战性的句子顺序预测（SOP）任务，其中通过交换两个相邻句子的顺序来构建否定样本，而不是从其他文档中采样随机句子。

与此相关的是，Raffel等人（2020）将监督的下游样本如GLUE（Wang et al.，2018）混合到T5的预训练数据集中，以进行多任务预训练。然而，值得注意的是，由于它们将所有任务统一为文本到文本的格式，因此训练目标是相同的

### Implementation Design
虽然大多数关于预训练语言管理模型的研究都集中在设计训练目标上，但多年来，为了追求稳定性、性能和效率，Transformerarchitecture本身的低级别实现也在不断改进。Vaswani等人（2017）提出的原始变压器块公式为ash=LN（注意力（x）+x），（4）y=LN（FFN（h）+h），（5）其中，层的输入是层的输出，“注意力”是自注意力子层，“FFN”是前馈子层，而“LN”是层非恶意化（Ba等人，2016）。GPT-2（Radford等人，2019）将层非恶意化移动到每个Transformer子块的输入，以稳定训练：h=注意力（LN（x））+x，（6）y=FFN（LN（h））+h，（7）

GPT-J（Wang和Komatsuzaki，2021）修改Transformer块以并行计算FFN子层和自注意子层，以增加计算吞吐量：y=x+FFN（LN（x））+注意（LN（x）），（8）和Chowdhery等人（2022）在将该设计应用于大型机模型时观察到有限的性能退化。


PaLM（Chowdhery et al.，2022）将Ro tary Position Embedding（RoPE）和Multi-QueryAttention（MQA）引入LLM。RoPE（Su et al.，2021）将每个自关注层的关键字和查询乘以位置相关的旋转矩阵以注入位置信息，并且稍后能够实现用于处理较长序列的位置插值（Chen et al.，2023；Roz-ière et al.，2022）。作为RoPE的替代方案，Press等人（2022）提出了ALiBi，它根据键和查询之间的相对位置直接衰减注意力得分。这种位置嵌入编码方案后来被BLOOM采用（Scaoet等人，2022）。

除了位置嵌入，Transformer中长期困扰研究人员的另一个问题是，自注意的复杂性与输入序列长度成比例关系。一些作品，如Reformer（Kitaev et al.，2020）、Lin former（Wang et al.，2019）、Performer（Choroman ski et al.，2021）和cosFormer（Qin et al.，2022），使用了近似的注意力来减少这种复杂性，但它们大多是以性能下降为代价的。其他作品从工程学的角度解决了这个问题。MQA（Shazeer，2019）在所有注意力头上共享相同的一组密钥和值，以优化内存与算法的比率，并以较小的模型性能成本显著提高推理速度。其多样化的分组查询注意力（GQA，Ainslie et al.，2023）采取了一种中间立场的方法，将注意力头分为多组，并在每组中共享相同的密钥/值集。正交地，Dao等人（2022）介绍了FlashAttention，这是一种精确但改进的自我注意力实现，通过平铺优化加速设备上的IO操作，以提高内存效率。

## Evaluation of Language Models for Code
在过去的十年里，软件工程社区提出了各种评估任务来评估代码模型。CodeXGLUE（Luet al.，2021）将大多数此类任务合并为一个单一的基准，涵盖代码理解任务，如克隆检测、缺陷检测，以及序列到序列的生成任务，如代码修复、代码翻译、程序合成和代码摘要。然而，在Chen等人（2021）引入HumanEval和Codex之后，文本到代码合成在NLP社区中受到关注，并成为评估LLM的标准任务（图2）。因此，我们首先在§3.1中简要介绍了每个传统任务以及预训练语言模型在其中的应用，并在图3、4中提供了每个任务的相关工作的综合列表。然后，我们在§3.2中回顾了评估度量，并在§3.3中更详细地研究了程序综合。最后，我们还在§3.4中讨论了存储库级别评估的最新趋势。在Ap pendix A中，我们列出了每个下游任务的基准。

### Downstream Tasks of Code Pressing
根据软件工程中的自定义，我们根据代码的输入/输出模式对代码的评估任务进行分类，并将这些任务划分为五个系列：文本到代码、代码到代码、码到文本、码到模式和文本到文本。我们注意到，这种分类法与NLP中的理解-生成二分法交织在一起，因为每个类别都可能包含理解和生成任务，如§3.1.6所述。

#### Text-to-Code
文本到代码任务将文本作为输入，并输出代码。

- _Code Retrieval_ 旨在检索相关的给定代码的自然语言查询，或从未注释的语料库中挖掘并行文本代码对。这项任务通常是通过计算查询和candi日期代码嵌入之间的相似性来执行的，双向语言模型（如BERT）产生的上下文嵌入已被证明是非常有用的。Grazia和Pradel（2023）以及Xie等人（2023。
-  _Code synthesis_ 是指在给定自然语言描述的情况下生成代码（通常是一个函数或方法）。这项任务可以看作是对使用生成模型而不是检索模型进行代码检索的更新。统计机器翻译（SMT）和神经机器翻译（NMT）模型已被广泛采用，通常具有利用编程语言的独特语法规则的增强解码器（Dong和Lapata，2016；Yin和Neubig，2017）。然而，基于 Trans-former 架构的预训练语言模型通过以自回归语言建模风格直接生成源代码来改变游戏规则，甚至无需针对特定任务进行微调（Chen 等人，2021）。 我们将在第 3.3 节中更详细地讨论此任务。
-  _Text-to-SQL_ 是代码合成的一种特殊情况（可以说更容易），其中模型的任务是从自然语言查询中生成SQL命令。由于SQL的结构化特性（与Python和C等通用语言相比）以及在数据管理中的广泛应用，它一直是一个特别有趣的话题。我们参考Kumar等人（2022）和Deng等人（2022年）对这一主题的调查。
-  _Math programming_ 也是代码合成的一种特殊情况，其中需要一个语言模型来通过将由外部interpreter执行的生成代码来解决数学推理问题。这项任务将推理过程从数值计算中抽象出来，因此对评估LLM特别感兴趣。

#### Code-to-Code
代码到代码的任务将代码作为输入，并输出代码。
- _Code search_ 是一项类似于代码检索的任务，与后者的不同之处在于，输入是现有的代码片段，通常使用与目标不同的编程语言。
- _Code completion_ 是指在给定前缀的情况下完成一段代码。这本质上是应用于代码的语言建模，相关技术已逐步引入：n-gram、RNN和Transformer。然而，由于编程语言的结构化性质，许多早期工作发现语法辅助的统计模型表现更好（Bielik et al.，2016；Hel lendoorn和Devanbu，2017），神经模型在2018年后才占主导地位（直观概述见图3）
- _Code translation_ 是指将一段代码（通常是一个函数或方法）翻译成另一种编程语言。代码翻译和跨语言代码搜索之间的关系类似于代码合成和文本到代码检索之间的关系，SMT/MNT模型也被广泛应用于这项任务。代码合成有助于帮助程序员编写代码片段，与此不同，代码翻译是迁移用obsolete语言编写的旧项目的重要技术。然而，我们还没有看到这样的应用程序，因为事件的上下文窗口——面对这样的项目，最强大的语言模型是非常有限的。
- _Code repair_ 也称为bug修复，旨在修复一段有缺陷的代码。与代码翻译一样，这是一项传统的序列到序列生成任务，关于这一主题的调查非常丰富（Gazzolaet al.，2018；Monperrus，2018；钟等人，2022；张等人，2023；Huang等人，2022）。
- _Cloze test_ 在BERT式预训练兴起后，克隆睾丸是最近提出的一项代码处理任务。由于编程语言的独特语义，该测试通常会选择几个关键词，如asminadmax（Feng et al.，2020）。需要重新修复<!-- 
- _Code filling_ 在填充中间预训练（Bavarian et al.，2022）变得流行之后，代码填充是最近提出的另一项任务。它是代码补全的一个推广，不仅给出了左上下文，还给出了右上下文。然而，它与完形填空的不同之处在于，完形填空目标只有一个标记，而代码填充的目标可以是整行甚至多行，这需要解码器自回归生成。
- _Obfuscation_ 混淆是指将标识符（如变量、方法和类）重命名为var_1、var_2或者x、y等通用名称的过程。它是病毒检测、知识产权保护和代码大小缩减的一项重要技术（Collberg和Thompson，2002；Murad等人，2010；Vasilescu等人，2017）。去模糊处理指的是反向过程，从模糊处理的程序中恢复有意义的标识符名称。模糊语言模型的应用很少，因为它可以很容易地静态实现，但近年来去模糊一直是人们更感兴趣的对象，并已被用作代码语言模型的预训练目标（Lachaux et al.，2021；Ding et al.，2022）。
- _Unit test_ 单元测试生成旨在为给定程序生成单元测试。在Codex和其他代码LLM兴起之前，该领域的几乎所有工作都采用了非神经方法（见图3）。然而，在LLM时代，这项任务变得越来越重要，因为研究表明，目前用于评估LLM程序可综合性的单元测试可能不够（Liu et al.，2023）。
- _Assert_ 断言生成是一项与单元测试密切相关的任务。给定一个程序和一组单元测试，此任务旨在生成断言（也称为软件工程中的断言），以使用单元测试评估程序。这项任务通常没有被NLP社区注意到，因为用于评估LLM的程序合成任务通常涉及独立的竞争式方法，对于这种方法，简单地断言程序输出和预期答案之间的相等性就足够了。
- _Mutant generation_ 突变体生成是为了进行突变测试而生成给定程序的突变体，它与单元测试生成和断言生成密切相关。一组给定的单元测试和断言未检测到的突变表明需要额外的测试用例或更好的断言（Fraser和Arcuri，2011）。最近，屏蔽源代码中的标记并从屏蔽语言模型的输出中对其进行采样已成为该任务的常用方法。Papadakis等人（2019）对蓟属植物进行了调查。
- _Fuzzing_ 旨在变异一组给定的单元测试以生成新的测试用例，这是与测试软件相关的另一项任务。虽然最近许多关于模糊测试的工作都针对深度学习库，但很少有人利用语言模型来执行此过程。
- _Type prediction_ 类型预测旨在预测Python和JavaScript等动态编程语言的类型。它已被用作代码语言模型的预训练目标（Wang et al.，2022），其中它通常被简化为二进制标记任务，以预测代码中的哪些标记是标识符（Wang et al，2021；Wang et al.。2021）


#### Code-to-Code

- _Code Summarization_,也称为文档字符串生成，旨在为给定的代码（通常是函数或方法）生成自然的语言描述。这与代码合成相反，SMT/NMT技术也得到了同样的应用。张等人（2022）对此主题进行了调查。
- _Code_review_ 代码审查旨在自动化每个代码审查的过程，可能有多种形式。许多早期的工作将其表述为一个二元分类任务，在提交时接受或拒绝更改，而其他工作则利用信息检索技术从现有评论库中推荐评论。然而，随着生成模型的能力越来越强，研究人员也研究了直接生成评论作为序列到序列的学习任务。
- _Identifier prediction_ 标识符预测是在代码中预测标识符名称的任务。由于这些名称似乎包含重要的语义信息，该任务已被用于代码汇总（Allamanis et al.，2016）以及预训练代码模型（Wang et al.，2021；Niu等人，2022）。

#### Code-to-Patterns

- _Defer dectection_ 缺陷检测预测输入代码是否有缺陷，是一项标准的单句分类任务。

- _Clone detection_ 克隆检测预测两段代码是否是彼此的克隆。在软件工程中，存在四种类型的代码克隆，最难识别的类型是语义克隆，即具有相同功能的语法上不同的代码。由于该任务可以被视为一个两句话分类任务，因此BERT风格的语言模型已被广泛应用于
- _Code Classification_ Mou等人推广的代码分类（2016）旨在预测预定义标签集中一段代码的功能。一个非常相似的任务是作者识别，它预测输入代码的作者。这两项任务都是标准的单句分类任务。
- _Code reasoning_ 代码推理是最近引入的评估LLM的任务，通常作为通用评估基准的子集，如MMLU（Hendrycks et al.，2021）。这项任务要求模型对代码或算法进行推理，并回答以多选形式编写的相关问题，这些问题可能从概念理解到数值计算和复杂性分析
  

#### Text-to-Text
- _Doucment translation_ 文档翻译是代码相关文档的自动翻译。由于机器翻译的模型、数据集和提示策略在NLP中非常丰富（Vaswani et al.，2017；Goyal et al.，2022；He et al.，2023），我们不详细介绍这项任务。
- _Log parsing_ 日志解析旨在分析软件产品生成的系统日志，例如将日志解析为结构化模板或从原始日志中发现异常。朱等人（2019）对截至2018年的传统方法进行了调查，而张等人（2023）也涵盖了更新的方法。


#### NLP Point-of-View
在之前列出的任务中，代码合成、代码翻译、代码修复、去模糊、单元测试生成、断言生成、变异生成、模糊化、代码摘要、代码审查和标识符预测是序列到序列的生成任务。形式上，这些任务的每个实例都有一个源序列集（例如一段源代码）和一个目标序列集（如其相应的摘要），语言模型的任务是最大化（3）给出的条件概率，其中θ可以是仅解码器模型，也可以是编码器-解码器模型。在前一种情况下，xandy是串联的。在后面的情况下，xis由编码器处理，andy由解码器处理。

代码完成和代码填充也是生成任务，与（1）和（3）中给出的两个预训练目标完全对应，但代码填充只要求输入中的一个跨度。同样，完形填空也是一项理解任务，其形式与（2）相同。

缺陷检测、克隆检测、代码分类和类型预测是序列分类任务。在这些任务中，在输入上定义了一组标签Yi，并且每个实例都被分配了一个la bely∈Y（例如，对于缺陷检测Y={0,1}，而对于类型预测，可能的Yis{int，float，string，bool，others}）。然后，该模型被赋予最大化pθ（y|x）的任务。（9） 最后两个任务——代码检索和代码搜索——也属于理解任务。在这些任务中，每个源序列与一个正目标序列和一组负目标̄y∈{y1，··，yk}配对。该模型的任务是找到一个相似性度量，即（x，y）大于（x，̄y）。

### Evaluation Metrics
在§3.1中提到的任务中，理解任务在形式上与自然语言理解任务相似（Wang et al.，2018；Wang et al.，2019），并同样通过准确性、F1和平均倒数排名（MRR）等指标进行评估，而识别预测等短生成任务也通过行为匹配的准确性进行评估。代码到文本任务是用诸如BLEU之类的文本生成的通用度量来评估的（Papineni等人，2002）。

另一方面，涉及代码生成的任务的评估更为复杂。大多数早期作品都评估了语法的正确性，即可以成功分析的代的百分比。陈等人（2018）反对这种方法，而是建议参考匹配，即与参考完全相同的世代的百分比。Ren等人（2020）提出了CodeBLUE，这是BLEU的一种变体，通过评估抽象语法树（AST）和数据流的重叠，将代码语法和语义考虑在内。

然而，随着代码生成模型多年来变得越来越强大，这些基于内容重叠的指标被发现是不相等的（Rozière等人，2020；Hendrycks等人，2021；Austin等人，2021），因为功能上相等的代码片段在其词汇形式上可能存在巨大差异。因此，研究人员将注意力转向了功能正确性。此类指标的一个流行示例是pass@k，专业-,由Kulal等人提出。（2019），由Chene等人完善。（2021），这是模型通过任何生成样本的程序图的所有单位测试的机会的无偏估计量。这个度量可以推广到passn@k（Li et al.，2022），它限制了模型提交的数量，但允许通过样本输入中给出的单元测试进行过滤。

### Program Synthesis
随着代码模型的发展，研究人员逐渐将注意力转向程序合成的实际任务。CON-CODE（Iyer et al.，2018）是该领域的早期数据集之一，包括超过10万个Java方法，并作为CodeXGLUE基准的子网（Lu et al.，2021）。自2021年以来，社区见证了这项任务数据集的激增。它们中的大多数，包括APPS（Hendrycks et al.，2021）、Hu-manEval（Chen等人，2021）和MBPP（Austinet等人，2021年），都专注于python，但最近的工作也将HumanEval扩展到了其他编程语言中（Cassano et al.，2023；Zheng et al.，2022；Muennighoff等人，2023）。DS-1000是一个更现实的Python数据集，专注于NumPy和SciPy等数据科学库，同时一些数学推理基准也已转换为编程任务，包括MathQA Python（Amini et al.，2019；Austin et al.，2021）和GSM8K - Python

### Repository-Level Evaluation
§3.1和图3中讨论的大多数评估任务仅限于单个文件甚至单个函数，因为跨文件代码建模带来的挑战超出了大多数现有语言模型的能力。然而，最近，位置插值技术（Chen et al.，2023；Rozièreet等人，2023）已经将LLM的文本窗口扩展到数十万个令牌，从而有可能在整个存储库中对代码建模的评估进行上下文化。一些工作（Shrivastava et al.，2023；Ding et al.，2022；Zhang等人，2023）已经研究了代码完成杠杆的假设级上下文，Liu等人（2023）提出了RepoBeach来评估这类系统。最近，Bairi等人（2023）研究了API库级迁移和时间编辑的更具挑战性的任务，Jimenez等人（2022）提出了相应的基准SWE-bench。

## General Language Models for Code
由于语言模型扩展到数百个参数（Brown等人，2020；Chowdheryet等人，2022），其中许多模型已经证明了非平凡的编码能力，即使它们不是专门为代码设计或训练的。由Codex开创的研究人员还发现，持续的代码预训练可以显著提高语言模型在code4上的性能。

### Off-Shelf Lanaguage Models
大型语言模型通常根据缩放定律在三个标记上进行预训练（Kaplanet等人，2020；Hoffmann等人，2022），如此大量的文本数据通常是一个多样化的假设，其中包含不可忽略的代码部分。例如，ThePile（Gao et al.，2021）在其800GB的原始数据集中包括95GB的从GitHub抓取的代码，而多语言预训练数据集ROOTS（Laurençon et al.，2022）在其1.6TB的复合中也包含163GB的代码，跨越13种编程语言。作为两个最大的开源预训练数据集，它们支持许多具有编码能力的语言模型。例如，Chen等人报告了GPT-J（Wang和Komatsuzaki，2021）。（2021）证明了HumanEval的非平凡性能，而Scao等人（2022）报告了GPT NeoX（Black等人，2022）和BLOOM的相似结果。LLaMA（Touvron et al.，2023），其预训练数据集包括来自GitHu的328GB代码。

另一方面，闭源模型通常表现更好。 LaMDA (Thoppilan et al.,2022) 和 PaLM (Chowdhery et al., 2022) 的预训练数据集分别包含 12.5% 和 5% 代码，在 HumanEval 上实现 14.0 和 26.2 pass@1per-formance，而 GPT-4 (OpenAI,2023) ）创下了 67.0 的惊人记录（Bubeck 等人（2023）报告的早期版本为 82），直到最近，该记录仍然高于任何针对代码进行预训练或指令微调的专用模型。

最近，总体趋势是使用更大的数据集来构建更小的模型，遵循修订后的比例定律（Hoffmann et al.，2022）。例如，百川2号（Yang et al.，2023）是一个在2.6T代币上训练的13B模型，而Qwen（Bai et al.。2023）则是一个用3T代币训练的14B模型。他们分别达到17.1和32.3pass@1onHumanEval。李等人（2023），无论如何，证明了小到1.3Bc的模型可以获得与大得多的模型相比较的编码能力，同时也保持了在一般文本处理上的区域性性能，甚至表现出一些新兴的能力（Wei et al.，2022），如通信链（Wei等人，2022）。他们的模型Phi-1.5使用了ChatGPT生成的21B标记的教科书数据，以及来自Stack Overflow和精化web（Penedoet al.，2023）的100B标记的过滤web数据，并达到41.4pass@1performanceonHumanEval。这些模型的确切性能以Table1 表示。

### Language Models with Additional Pretraining on Code

与开创性的基准HumanEval一起，Chen等人（2021）用Codex开启了LLM代码的时代，Codex是在100B额外代码标记上预训练的GPT-3检查点，也是最早的数十亿代码模型之一。在他们的工作之后，其他研究人员也通过额外的预训练将LLM专门用于代码。Chowdhery等人（2022）在7.8B额外的代码令牌上训练PaLM以获得PaLM编码器，在Hu-manEval和MBPP（表1）上设置了新的最先进技术，这两个技术后来才被其继任者PaLM 2-S*（PaLM 2的最小版本）（Anil等人，2023）在未公开数量的代码上进一步训练。Sim ilarly，Lewkowycz等人（2022）在arXiv论文和数学内容的38.5B标记上训练PaLM，而Rozière等人（2023）在超过500B的代码上训练LLaMA2（Touvron等人，2023）以获得代码LLaMA，其在HumanEval上的性能超过了除GPT-4以外的所有先前的LMs（表1）。刘等（2023）f

虽然几乎所有这些模型都是用CLM预训练的Trans-former解码器，但正如我们在§2.5中所指出的，长期以来已经引入了一些结构修改。所有这些模块都使用预范数，GPT-J引入了并行注意力，后来被PaLM、GPT-NeoX和Phi-1.5采用。PaLM在LLM中引入了MQA和RoPE，RoPE现在被大多数语言模型所使用，包括GPT-NeoX、两代LLaMA、Qwen和7B版本的百川2。然而，BLOOM和白川2号的13B版本使用ALiBi定位床，而LLaMA 2和代码LLaMA采用GQA而不是MHA或MQA。在§5中，我们展示了专门在代码上预训练的专业模型也紧随这些进步。

## Specialized Language Models for Code

随着GPT和BERT等经过预训练的转换器在自然语言处理方面取得了显著的成功，这些模型架构、学习范式和训练目标很快就实现了,被软件工程社区采用，以产生用于代码理解和生成的专门模型。在本节中，我们首先重新查看用于预训练代码语言模型（§5.1）的常见数据集，然后根据它们的模型体系结构深入到复杂的代码LM族：仅编码器模型（§5.2）、编码器-解码器模型（§5.3）、仅解码器模型（节5.4）、UniLM（§5.5）和扩散模型（§5.6）。最后，在§5.7中，我们还说明了在NLP中应用最新技术的当前趋势，如结构调整（Wei et al.，2022；Sanh et al.，2020；Chung et al.，2021）和强化学习（Ouyang et al.，2018）到代码处理。表3提供了这些预训练模型的概览。

### Training Dataset for Code
虽然用于预训练语言模型的文本数据通常是从网络上抓取的，并且必须经过细致且经常是激进的预处理（Raffel et al.，2020），但代码数据自然地作为整个文档来自公共GitHub仓库。更好的是，它们提供了现成的质量指标，如星叉数量（尽管Allal等人（2023）认为星叉数量与下游性能的相关性很差）。因此，引入了许多大规模的代码预训练数据集，包括CodeSearchNet（Husain et al.，2019）、CodePar-rot（Tunstall et al.，2022）和Stack（Kocetkov et al.，2021），总共分别为20GB、50GB和3TB的代码文档（表2）。

虽然这些数据集是用来训练代码模型的，但应该注意的是，代码最终是自然语言的一种形式，因为大多数编程语言的词汇都是英语的一个子集。此外，高质量的代码通常与自然语言注释或文档交织在一起，这也使模型能够获得通用文本表示的某些知识。事实上，在CodeSearchNet中的6.5M个函数中，2.3M个与自然语言文档配对，允许模型在这种双峰数据上进行显式训练。

与自然语言相比，从GitHub中抓取代码的另一个副产品是com mit history，它由com mit之前的代码、提交之后的代码和一条短消息组成，它可以松散地作为语言模型的指令。Muennighoff et al.利用这一特性，构建一个2GB数据集CommitPackFT，其中包含742K个代码指令数据样本，消除了构建自然语言指令所需的大量人力（Sanh et al.，2022；王等人，2022）。

除了双峰训练和指令微调之外，构建代码数据集的另一个最新趋势是使用强大的模型（如ChatGPT）合成数据。虽然这种方法最初是为生成自然语言的教学数据而提出的（Wang et al.，2023；Honovich et al.，2021），Gunasekar et al.（2023）更进一步，合成了Python教科书和代码练习的1B标记，以预训练1.3B模型，在HumanEval上实现了最先进的结果，可与在更大的数据集上训练的更大的模型相比较。

### Encoders
BERT（Devlin et al.，2019）、RoBERTa（Liu et al.，2017）和ELECTRA（Clark et al.，2020）等预训练转换编码器在自然语言理解任务上取得了令人印象深刻的成果，这些方法出现后很快被引入代码处理。Kanade等人（2020）在代码库上复制了BERT的训练过程，以产生CuBERT，展示了其优于LSTM（Hochreiter和Schmidhuber，1997）和未经预训练的Transformers的性能。另一方面，
Feng et al. 在CodeSearchNet上用MLM和ELECTRA的RTD训练CodeBERT。他们还利用CodeSearchNet中的显式文本代码对，并将它们分别用作BERT输入中的第一段和第二段。当使用CodeBERT初始化普通Transformer的编码器部分以执行序列到序列的生成任务（如代码摘要）时，他们在预训练的基线上观察到适度的性能增益。

除了这些标准的培训目标外，还引入了许多专门为代码设计的辅助目标。GraphCode BERT（Guo et al.，2021）和SynCoBERT（Wang et al.，2022）都从源代码中提取图（分别为数据流图和抽象语法树），并训练模型来预测节点之间的拓扑关系，而Syn-CoBERT和Code MVP（Wang等人，2022）也以标记的形式将类型推理添加到其预训练阶段。另一个共同的目标是集中学习：SynCoBERT和Code MVP在输入的不同视图（如代码、注释、AST和转换代码）之间进行对比，而DISCO（Ding et al.，2022）通过模糊等语义保留转换构建正样本对，通过注入人工错误构建负样本对。


### Encoders-Decoders
在 NLP 中，预训练的 Transformer 编码器-解码器，例如 T5 (Raffel et al., 2020) 和 BART (Lewis)在过去几年语言建模的进步中也留下了显着的印记。例如，T5 将所有文本任务统一为序列到序列的格式，并在 GLUE（Wang et al., 2018）和 SuperGLUE（ Wanget 等人，2019）。与仅编码器模型相比，编码器-解码器自然更强大，因为它们可以用于条件文本生成，而它们的编码器部分始终可以被取出来执行需要仅编码器架构的任务，例如回归（Tay et等，2023）。

受编码器-解码器体系结构的这些优点的启发，许多这样的模型已经被提出用于代码处理。PyMT5（Clement et al.，2020）和Mastropaolo et al.（2021）复制了T5on代码语料库的预测和多任务微调过程，而Ahmad et al.（2020）介绍了PLBART，这是一种在Java、Pyhton和自然语言的655GB组合数据上预训练的BART。Lachaux等人（2021）认为，对于编程语言来说，传销可能太容易了，因为标识符名称经常在一个上下文窗口中出现多次，并提出了一个去模糊预训练目标，在该目标中，模型被训练以将模糊代码转换回其原始形式。与该方法相关，我们注意到，有意义的富变量名称也被发现对大型语言模型的代码生成过程有积极影响（Chen et al.，2022）。


在这些早期工作的基础上，王等人（2021）提出了CodeT5，它与1）T5的原始跨度腐败进行了交替的预训练；2） 标识符标记（其中代码输入中的每个令牌被标记为标识符或非标识符）；3） 屏蔽标识符预测（一种特殊形式的扫描破坏，其中所有标识符都被屏蔽）；以及4）文本到代码&amp;代码到文本生成。它的访问者CodeT5+（Wang et al.，2023）从UL2（Tay et al.，2021）中吸取了经验教训，并在预训练中引入了配偶语言建模（CLM），以及基于文本代码匹配的额外对比目标。

AlphaCode（Li et al.，2022）也使用多个目标进行训练，其中编码器使用MLM进行管理，解码器使用CLM进行训练，并对架构进行了修改，如浅编码器和深度解码器、多查询注意力（Shazeer，2019），并且比CodeT5大得多（高达41B参数）。另一方面，Nat-Gen（Chakraborty et al.，2022）是用类似于去模糊的“自然化”目标进行预训练的：通过预定义的操作（如循环变换、死代码输入和变量重命名）生成语义等效但非自然的代码，并预训练模型将这些非自然代码翻译回其原始形式。我们注意到，其中一些模型是建立在以前的工作基础上的。例如，NatGenis用CodeT5初始化，而CodeT5+的最大版本是从仅解码器的模型CodeGen初始化的（Nijkamp等人，2023）。


除了这些一般的预训练目标外，一些工作还训练了Transformerencoder解码器，重点是代码翻译，这是Transformer模式在代码中的自然应用，因为Transformer架构最初是由Vaswani等人提出的。（2017）形式机翻译（MT）。然而，与自然语言不同的是，在自然语言中，存在大量跨两种或多种人类语言的并行语料库，几乎没有用于代码的并行数据。为了解决这个问题，Rozière等人（2020）提出了Transcoder，它首先用XLM预训练编码器（Conneau和Lample，2019），然后用这个编码器初始化一个普通的Trans-formar，并继续用去噪自动编码（DAE，Lewis et al.，2020）和反翻译（Sennrich et al.，2016）预训练它，而它的后续工作（Szafraniec et al.，2023）也利用独立于语言的中间表示来增强这一过程，我们在§6中对此进行了更详细的讨论。

除了训练数据和目标外，这些模型大多保持NLP社区提出的原始架构，如表3所示。例如，基于BART的模型使用后归一化和可学习的绝对位置嵌入，而基于T5的模型使用其简化的相对位置嵌入和预归一化。

### Decoders


尽管Christopoulou等人（2022）报告去噪目标在仅解码器模型中表现不佳，但也有其他工作成功地将去噪或多任务预训练与解码器架构完全结合。Incoder（Fried et al.，2023）、SantaCoder（Allal et al.，2021）和Star-Coder（Li et al.，2020）都是用填充-中间（FIM）目标进行训练的，也被Fried et al.（2023）称为ascausal掩蔽，其本质上跨越了用于仅解码架构的腐败（Raffel et al.，20）。这些填充目标的一个明显优势是，它们为模型注入了在推理时填充输入代码中间的空格的能力，而CLM只允许自回归生成。然而，如表4所示，与仅使用CLM的模型（如CodeGen）相比，这些目标在下游任务上会带来更高的性能。

观察表3，很明显，与其他模型架构相比，仅解码器的代码模型通常更紧密地遵循了NLP中的实践。所有这些模型都使用了预归一化，而MQA、RoPE和并行注意力也被几个模型所采用。值得注意的是，最近的三种型号——StarCoder、Phi-1和CodeFuse——也采用了FlashAttention来提高模型吞吐量。

### UniLMs
继NLP中的UniLM（Dong et al.，2019）之后，代码处理中的一些工作也对第四族Transformer模型进行了代码预训练。CugLM（Liu et al.，2020）通过交替的注意力掩码用CLM和MLM+NSP进行训练，而UniXcoder则用CLM、MLM、SpanCorporation（前缀LM风格）以及包括对比学习和文本代码相互生成在内的辅助目标进行训练。然而，这两个模型的规模都相对较小，这种体系结构是否适合代码处理还有待探索。

### Diffusion Model
目前，Transformer架构主要用于文本生成，但有几项工作（Li et al.，2022；Lin et al.，2023）也采用了计算机视觉中的文本扩散模型（Ho et al，2020）,一代最近，CodeFusion（Singh et al.，2023）也将扩散模型引入到代码建模中，并证明75M扩散模型在3个代码合成数据集上可以优于StarCoder、CodeT5+和GPT-3。

### Instruction Finetuning and Reinforcement Learning for Code
在自然语言处理中，具有指令前缀的不同任务集的训练模型，即已知的指令微调，已被证明可以释放跨任务泛化的能力（Ouyang et al.，2022；Chung等人，2022；Iyer等人，2022）。起初，这些指令数据样本是手动编译或众包的（Wei等人，2022；Sanh等人，2022），但后来的研究发现LLM生成的指令是足够的（Wang等人，2023；Honovich等人，2022）。

在自然语言中完成这些工作之后，代码社区的重新搜索者也将结构调整应用到了他们的模型中。Wang等人（2023）用InstructionGPT生成的20K指令内数据对CodeT5+进行微调（Ouyang et al.，2022），以获得InstructionCodeT5+。Wizard Coder（Luo et al.，2023）遵循WizardLM（Xu et al.，2021）的方法，将20K编码的al-paca（Taori et al.，2022）样本进化为78K数据集，并使用它来微调StarCoder。Pangu-Code2（Shen et al.，2023）还使用WizardLM的Evol instruction从20K代码Alpaca中生成68K指令样本，但也引入了通过等级响应来调整测试和教师反馈（RRTF）的强化学习。另一方面，OctoCoder（Muen-nighoff et al.，2023）采用不同的路径，并使用Git提交历史作为指令数据来微调StarCoder和CodeGeeX2。最近，CodeFuse（Di et al.，2021）还采用了多任务微调，并在其指令数据中显式引入了多个下游任务。这些指令的性能微调代码模型也可以在表4中找到。


在NLP中，另一项与结构中微调密切相关的技术是来自人类反馈的强化学习（RLHF），它在使LLM与人类价值相一致方面发挥了重要作用（Ouyang et al.，2022；Bai等人，2022）。强化学习的优点是，它可以将不可微分的奖励信号纳入训练中，如BLEU（Bahdanau et al.，2017）和人类偏好（Christiano et al.，17），但调整LLM所需的人类反馈往往涉及大量的注释工作。相比之下，将强化学习应用于代码模型具有天然的优势，因为编译器可以用于自动生成语言模型生成的代码样本的反馈。

CodeRL（Le et al.，2022）就是这样一个模型，它为每个生成的程序定义了四个级别的奖励（即编译错误、运行时错误、单元测试失败、通过），以及由关键模型估计的细粒度令牌级别的奖励。演员模型是CodeT5的扩展，然后使用增强算法进行训练（Williams，1992）。类似地，Comp编码器（Wang et al.，2022）和PPOCoder（Shojaeeet al.，2023）分别通过近端策略优化来训练CodeGPT和CodeT5（Schul-man et al.，2017），而RLTF（Liu et al.，2021）则提出了基于编译器提供的错误信息和位置的细粒度反馈，以及考虑已处理测试用例比例的自适应反馈。

## Code Features for Language Models
编程语言和自然语言之间的一个主要区别是，前者被人工定义为精确和明确的，并且需要在执行之前进行编译（或解释）而不会出现错误。除了CLM、MLM和Span Corruption等词汇操作之外，这允许在设计代码上的预训练目标时具有更大的灵活性。在神经网络工作被引入主流NLP之前的最后几年，也可以观察到类似的趋势（Sutskever et al.，2014；Bahdanau et al.，2015），当时MT社区的研究人员利用句法特征等文本的替代观点来提高SMT系统的性能（Galley et al.，2006；Chiang，2007）。然而，这些特征并不是普遍适用的，甚至不是一致同意的，而且往往会导致高度复杂的系统（例如，英语词性标签的标签集大小可能在几十到几百之间）。

然而，编程语言在这些方面做得更好。每一种主流的编程语言，如C、Python和Java，都有现成的编译器工具包，可以轻松准确地提取信息中的语义，如抽象语法树（AST）、独立于语言的中间表示,（IR）以及诸如每个令牌的类型和控制/数据流图（CFG/DFG）之类的辅助信息。因此，在基于Transformer的代码语言管理建模的背景下，许多工作已经将这些特性纳入了它们的训练过程中。

### Abstract Syntax Tree and Intermediate Representation

AST是编译过程中最常见的中间结果之一，在编译过程中，程序被分解为运算树及其操作数。在Transformer在代码处理社区普及之前，已经有像InferCode（Bui et al.，2021）这样的工作，它使用特殊的网络架构（如基于树的CNN）来处理这些表示，并通过预测子树来进行自监督预训练。

TreeBERT（Jiang et al.，2021）是将AST引入基于Transformer的跟踪微调框架的首次尝试之一。它是一个使用Tree MLMand节点顺序预测预训练的Trans-former编码器-解码器，其中编码器在输入时在AST中启动一组组成路径（每个令牌都是一条路径，这是其节点表示的链接）而解码器将代码作为输入。树MLM是通过屏蔽路径表示中的某些节点及其解码器输入中的相应代码标记来执行的，而节点顺序预测是通过交换路径中的节点并用类似于BERT的[CLS]标记进行预测来实现的。

然而，TreeBERT使用的方法很复杂，不能很好地扩展。后来的工作大多选择先将AST处理成文本序列，并将其视为输入的正常部分。例如，Wang等人（2021）以深度优先遍历的方式处理AST，并将其与代码和组件连接起来，然后训练SynCoBERT（与TreeBERT不同，它实际上是一个类似BERT的仅编码器模型），目标有四个：1）传销；2） 标识符标记；3） AST边缘预测（根据两个AST节点的表示的点积来预测这两个节点之间是否存在边缘）；以及4）对i）代码和AST对以及ii）文本和代码AST对的对比学习。类似地，SPT Code（Niu et al.，2022），一种Transformer编码器-解码器，以代码的连接、顺序化的AST和文本作为输入，并通过1）跨度破坏进行预训练；2） 编码AST预测（其中一个片段是编码的并且一个片段为AST的NSP）；和3）方法名称生成，这是一种特殊形式的跨度破坏，其中方法名称被屏蔽。然而，与其他作品不同的是，他们并没有将文档字符串作为输入中的文本段，而是将代码中出现的所有方法名称连接起来，作为一个偶然的自然语言描述。同样，UniXcoder（Guo et al.，2022）采用扁平AST


在编译管道中，AST通常由独立于语言的中间表示来遵循，例如LLVM IR（Lattner和Adve，2004）。这些功能与特定编程语言的独立性使其适合作为翻译支点，低资源自然语言的英语机器翻译也是如此（Leng et al.，2019）。Szafraniec等人（2023）利用这一特性，通过对代码和IR的翻译语言建模（Conneau和Lample，2019），以及从代码生成IR，扩展了Transcoder（Rozière等人，2020）。他们还研究了其他目标，如IR反编译（即从IR生成代码）和IR枢轴（即从另一种语言的IR直接生成一种语言中的代码），这两个目标都显示出了有希望的结果。

### Control Flow and Data Flow

虽然AST和IR已被证明在某些任务（如代码翻译）中是有用的信息，但它们本质上是静态的，就像源代码一样，可能无法捕获仅在运行时显示的代码的语义属性（Wang和Su，2020）。然而，这种语义包含在诸如控制流和数据流之类的动态特性中。与AST类似，在预训练的变压器出现之前，使用专门的网络来处理这些信息，例如ProGraML使用的消息传递神经网络（Cummiset等人，2021）。然而，与AST不同的是，即使在经过预先训练的变形金刚占据主导地位之后，也很少有作品朝着这个方向发展。


GraphCodeBERT（Guo et al.，2021）就是其中之一，它为流图中的变量创建了特殊的标记和位置嵌入，并在文本和源代码之后连接变量序列来构建模型输入，在代码和变量段上有定制的注意掩码：当且仅当从代码标记中识别变量时，代码段和变量段中的标记才能相互关注，并且允许在变量段内使用标记,为了关注vjif，数据流中存在来自vjtov的直接边缘。然后，将MLM与边缘预测和节点对齐相结合，对模型进行预训练，这两种方法都是通过从两个标记的表示的点积进行二进制分类来实现的（一个来自代码段，一个来自用于节点对齐的可变段，以及两个来自用于边缘预测的可变段）。

### Type
除了AST、IR和数据流之外，类型信息还用于帮助语言模型处理代码。例如，CugLM（Liu et al.，2020）在微调过程中使用类型信息来帮助预测单向MLM（即具有单向注意力任务的MLM）的令牌：首先从最终Transformer层的表示预测掩蔽令牌的类型，然后基于隐藏表示和预测类型来预测令牌本身。相反，CodeT5（Wang et al.，2021）和Syn-CoBERT（Wang et al.，2021）在其预训练目标中都包括标识符标记，这可以被视为粗粒度类型预测。

值得注意的是，Wang等人（2022）通过标识符重命名、循环交换和死代码插入，将许多上述特征集成到代码MVP中：源代码、文档字符串、AST、CFG和转换源代码。该模型从GraphCodeBERT初始化，然后通过MLM、细粒度类型预测和跨不同视图（如文本VS）的对比学习进行训练。代码、代码与AST以及代码与CFG。

## 7 LLMs in Software Developement
随着语言模型在软件工程基准上创下新纪录，作为回报，软件工程技术也在扩展语言模型的边界，并随后将其引入现实世界的开发周期。

### 7.1 LLMs Extended with Coding Tools
NLP社区的研究表明，LLM可以学习使用外部工具，如计算器、MT系统和搜索引擎（Thop-pilan等人，2022；Schick等人，2023）。因此，解释器已被用于增强LLM不复杂的推理任务。PAL（Gao et al.，2023）和PoT（Chen et al.，2022）都使用Python解释器扩展了Codex用于数值计算，而ViperGPT（Surís et al.，2021）通过调用视觉API从视觉输入中提取信息并回答相关问题来进一步扩展Codex。

除了减轻抽象推理任务中的数字计算负担外，inter-prepter还提供代码生成过程本身的反馈，以及单元测试。CodeT（Bareißet al.，2022）和TiCoder（Chen et al.，2023）使用Codex生成单元测试，这些测试针对生成的代码样本运行，以提高模型在代码合成方面的性能。Sim ilarly，TransCoder ST（Rozière et al.，2022）将TransCoder和DOBF与用于代码翻译的外部单元测试结合起来。在§5.7中，我们还表明，单元测试的执行结果是代码强化学习的自然监督信号。

值得注意的是，2023年3月，OpenAI还发布了ChatGPT8的解释器插件，该插件可以接受用户的文件输入，根据用户的指令生成代码，并通过实时执行提供反馈。周等人（2023）表明，该功能允许GPT-4进行自调试

与LLM研究中的工具使用密切相关的一个主题是将其规划为智能代理，这已被证明可以在理论和经验上增强LLM的能力（Feng et al.，2023）。Ruan等人（2023）发现LLM可以计划使用外部SQL生成器和Python生成器来解决复杂任务，而CodePlan（Bairi等人，2023）证明了它们可以通过自适应规划执行存储库级编码。

另一个工作流使用LLM创建用于代码生成的多代理系统，如自协作（Dong et al.，2023）、Chat-Dev（Qian et al.，2021）和MetaGPT（Hong et al.，2020）。在这些框架中，多个LLM被提示扮演不同的角色，如程序员、评审员和经理。这些角色相互作用，将代码生成分解为不同的阶段（例如设计、编码、测试和文档），并协作完成复杂的任务。

### 7.2 LLMs Integerated into Software Development

随着LLM的交互式编码能力的增强，研究人员也开始将其融入到软件开发的每一个过程中。

自动代码完成是语言模型在软件开发中最早的应用之一，因为它们只需要预测下一个标记的能力。甚至在语言模型扩展到数十亿参数之前，就已经将Pythia（Svyatkovskiyet et al.，2019）和IntelliCode（Svyatcovskiy et al.，2020）等计算系统集成到流行的IDE中。

然而，最近，代码语言模型的应用已经超越了简单的代码完成。GitHub Copilot可以说是最流行的人工智能代码助手之一，具有多种功能，包括代码生成、漏洞检测和许可证管理9，而CodeFuse（Diet等人，2023）还将代码生成、代码翻译、代码注释和测试用例生成集成到一个IDE扩展中。然而，随着代码语言模型变得越来越大，它们的客户端部署和实时性能也带来了新的挑战。

随着LLM的不断发展，在它们之上构建应用程序本身也在演变成一项必然的任务。许多用于此类应用程序的开源框架已经发布，包括LangChain10、AutoGPT11和WorkGPT12。这些框架为开发人员提供了对语言模型的抽象，并在本次调查最终确定之际，积极地改变了软件开发的整个过程。

## 8 Conclusion and Challenges
在这项工作中，我们系统地回顾了使用预训练的转换语言模型进行代码处理的历史，并强调了它们与在通用语言上预训练的模型的关系和比较。代码建模的发展大体上遵循了NLP的历史进程，从SMT模型发展到NMT模型，再到微调预训练的Transformers，最后到LLM甚至au-tonomous agent在现实世界生产中的少量应用。与自然语言不同，代码的性质使它可以很容易地从备选视图中提取辅助信息，并利用解释器和单元测试进行自动反馈。考虑到这些，我们确定了当前代码模型开发中的几个挑战。

- 将代码LLM推向下一阶段的全面基准测试。广泛使用的Hu-manEval基准在代码LLM的演变中起着关键作用。然而，它是相对完整的，它的记分牌被操纵得近乎完美，这并不能准确地反映现实世界的行为。已经提出了许多其他CodeLLM基准，但它们仍然不够全面，无法反映生产级别的要求。社区渴望在HumanEval之后建立一个新闻标准基准，以进一步推动Code LLM进入下一阶段。

- 获取高质量数据。随着Gu nasekar等人（2023）用在教科书数据上训练的1.3B模型实现SOTA性能，我们相信在不久的将来，训练数据的选择和合成数据的利用将更加突出，无论是在自我监督的预训练还是监督的微调中。

- 将代码特性集成到语言模型中。正如我们在§6.2中所指出的，CFG和DFG尚未在代码语言建模中大规模使用。少数使用数据流的工作对模型的注意力掩码进行了更改，这严重限制了它们的跨任务泛化和缩放能力。我们认为，将这些特征无缝集成到文本输入中是值得未来研究的。
- LLM在更多代码下游任务中的应用。正如我们在§3中所指出的，目前对LLM编码能力的评估主要集中在程序合成上，图3清楚地表明，与软件测试相关的任务（如单元测试生成、断言生成、突变生成和模糊化）和去模糊化很少应用LLM。此外，由于LLM的文本窗口目前非常有限，程序合成和代码翻译等生成任务还没有应用到方法级别之外。在§3.4中，我们列出了几个关于存储库级代码完成和临时编辑的工作，我们相信LLM在更多存储库级任务中的应用将成为未来的热门研究。
- LLM在更多代码下游任务中的应用。正如我们在§3中所指出的，目前对LLM编码能力的评估主要集中在程序合成上，图3清楚地表明，与软件测试相关的任务（如单元测试生成、断言生成、突变生成和模糊化）和去模糊化很少应用LLM。此外，由于LLM的文本窗口目前非常有限，程序合成和代码翻译等生成任务还没有应用到方法级别之外。在§3.4中，我们列出了几个关于存储库级代码完成和临时编辑的工作，我们相信LLM在更多存储库级任务中的应用将成为未来的热门研究。
- 为软件开发的整个生命周期构建代码LLM生态系统。虽然学术界已经见证了大量的代码模型，但大多数都是作为IDE插件部署在代码库中的，而忽略了软件开发生命周期中的其他阶段。在§7.2中，我们提到了几个鼓舞人心的例子，我们希望看到更多的代码LMST应用贯穿软件开发的整个生命周期，从需求分析到DevOps，甚至最终导致全面的生态系统，如PyTorch（Paszke et al.，2019）和HuggingFace13。
- 与代码LLM相关的安全和道德问题。随着语言模型的发展，它们也引发了安全问题，包括但不限于数据污染、有毒或有偏见的生成、个人信息泄露和幻觉。在软件开发中，应该格外小心地部署这些模型，因为它们生成的代码可能包含导致灾难性结果的安全风险。预训练数据也正在成为一个有意义的伦理话题，Kocetkov等人（2022）通过允许开发人员从堆栈中删除他们的代码，朝着这个问题迈出了有意义的一步。随着合成训练数据的广泛使用，搜索者也应该谨慎对待这种做法，因为用人工智能生成的数据训练人工智能模型的结果尚待大规模研究。


通过这项调查的介绍，我们希望提供一个语言模型在软件工程中应用的全球视角，并将这两个社区的研究联系起来。我们相信，当前LLM的激增将最终转化为现实世界的应用，并将人类带入更光明的未来。



